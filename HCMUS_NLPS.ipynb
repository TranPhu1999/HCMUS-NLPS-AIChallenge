{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TranPhu1999/HCMUS-NLPS-AIChallenge/blob/Phu/HCMUS_NLPS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7p7iuQEwbcY"
      },
      "source": [
        "# Tham khảo\n",
        "- Paper: https://paperswithcode.com/paper/frozen-in-time-a-joint-video-and-image\n",
        "- Towee repo: https://github.com/towhee-io/towhee\n",
        "- Video-text retrival: https://codelabs.towhee.io/how-to-build-a-text-video-retrieval-engine/index#1\n",
        "- Similar video: https://codelabs.towhee.io/build-a-reverse-video-search-engine-in-minutes/index#0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFAiErZ4uzTj",
        "outputId": "a4f43978-57fa-40a8-9a7f-5201d441d860"
      },
      "outputs": [],
      "source": [
        "# !pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Admin\\anaconda3\\envs\\env4ml\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import towhee\n",
        "import os\n",
        "import glob\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import unicodedata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Vietnamese Engslish Translate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "dict_map = {\n",
        "    \"òa\": \"oà\",\"Òa\": \"Oà\",\"ÒA\": \"OÀ\",\"óa\": \"oá\",\"Óa\": \"Oá\",\"ÓA\": \"OÁ\",\"ỏa\": \"oả\",\"Ỏa\": \"Oả\",\"ỎA\": \"OẢ\",\"õa\": \"oã\",\n",
        "    \"Õa\": \"Oã\",\"ÕA\": \"OÃ\",\"ọa\": \"oạ\",\"Ọa\": \"Oạ\",\"ỌA\": \"OẠ\",\"òe\": \"oè\",\"Òe\": \"Oè\",\"ÒE\": \"OÈ\",\"óe\": \"oé\",\"Óe\": \"Oé\",\n",
        "    \"ÓE\": \"OÉ\",\"ỏe\": \"oẻ\",\"Ỏe\": \"Oẻ\",\"ỎE\": \"OẺ\",\"õe\": \"oẽ\",\"Õe\": \"Oẽ\",\"ÕE\": \"OẼ\",\"ọe\": \"oẹ\",\"Ọe\": \"Oẹ\",\"ỌE\": \"OẸ\",\n",
        "    \"ùy\": \"uỳ\",\"Ùy\": \"Uỳ\",\"ÙY\": \"UỲ\",\"úy\": \"uý\",\"Úy\": \"Uý\",\"ÚY\": \"UÝ\",\"ủy\": \"uỷ\",\"Ủy\": \"Uỷ\",\"ỦY\": \"UỶ\",\"ũy\": \"uỹ\",\n",
        "    \"Ũy\": \"Uỹ\",\"ŨY\": \"UỸ\",\"ụy\": \"uỵ\",\"Ụy\": \"Uỵ\",\"ỤY\": \"UỴ\",\n",
        "    }\n",
        "\n",
        "def strip_accents(s):\n",
        "       return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "                  if unicodedata.category(c) != 'Mn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer_vi2en = AutoTokenizer.from_pretrained(\"../AIChallenge_data/ViEnTranslate/\", src_lang=\"vi_VN\")\n",
        "model_vi2en = AutoModelForSeq2SeqLM.from_pretrained(\"../AIChallenge_data/ViEnTranslate/\")\n",
        "\n",
        "def translate_vi2en(vi_text: str) -> str:\n",
        "    for i, j in dict_map.items():\n",
        "        vi_text = vi_text.replace(i, j)\n",
        "    input_ids = tokenizer_vi2en(vi_text, return_tensors=\"pt\").input_ids\n",
        "    output_ids = model_vi2en.generate(\n",
        "        input_ids,\n",
        "        do_sample=True,\n",
        "        top_k=100,\n",
        "        top_p=0.8,\n",
        "        decoder_start_token_id=tokenizer_vi2en.lang_code_to_id[\"en_XX\"],\n",
        "        num_return_sequences=1,\n",
        "    )\n",
        "    en_text = tokenizer_vi2en.batch_decode(output_ids, skip_special_tokens=True)\n",
        "    en_text = \" \".join(en_text)\n",
        "    en_text = strip_accents(en_text)\n",
        "    en_text = en_text.replace(\"\\\\\",\"\")\n",
        "    return en_text\n",
        "\n",
        "# vi_text = \"Cô cho biết: trước giờ tôi không đến phòng tập công cộng, mà tập cùng giáo viên Yoga riêng hoặc tự tập ở nhà. Khi tập thể dục trong không gian riêng tư, tôi thoải mái dễ chịu hơn.\"\n",
        "# print(translate_vi2en(vi_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFrsqOE1wvoP"
      },
      "source": [
        "# Towhee text-video inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhuwMvBMvWhO",
        "outputId": "a68ef35e-c0d6-4cb3-c894-7b3748247522"
      },
      "outputs": [],
      "source": [
        "# !curl -L https://github.com/towhee-io/examples/releases/download/data/text_video_search.zip -O"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ep6Mn3t0zR2s"
      },
      "outputs": [],
      "source": [
        "# Nếu chạy không được lệnh này thì ra ngoài folder rồi tự unzip\n",
        "# !unzip -q -o text_video_search.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtBQERZFzWcX",
        "outputId": "b65528db-5912-4690-89b2-aa7e37a311e3"
      },
      "outputs": [],
      "source": [
        "# ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "c-BaMxTszcxW",
        "outputId": "9c6ba52f-7817-4ce6-b64c-cbd20a356a6f"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import os\n",
        "\n",
        "# raw_video_path = './test_1k_compress' # 1k test video path.\n",
        "# test_csv_path = './MSRVTT_JSFUSION_test.csv' # 1k video caption csv.\n",
        "\n",
        "# test_sample_csv_path = './MSRVTT_JSFUSION_test_sample.csv'\n",
        "\n",
        "# sample_num = 1000 # you can change this sample_num to be smaller, so that this notebook will be faster.\n",
        "# test_df = pd.read_csv(test_csv_path)\n",
        "# print('length of all test set is {}'.format(len(test_df)))\n",
        "# sample_df = test_df.sample(sample_num, random_state=42)\n",
        "\n",
        "# sample_df['video_path'] = sample_df.apply(lambda x:os.path.join(raw_video_path, x['video_id']) + '.mp4', axis=1)\n",
        "\n",
        "# sample_df.to_csv(test_sample_csv_path)\n",
        "# print('random sample {} examples'.format(sample_num))\n",
        "\n",
        "# df = pd.read_csv(test_sample_csv_path)\n",
        "\n",
        "# df[['video_id', 'video_path', 'sentence']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RjbavLhRzkoT"
      },
      "outputs": [],
      "source": [
        "# from IPython import display\n",
        "# from pathlib import Path\n",
        "# import towhee\n",
        "# from PIL import Image\n",
        "\n",
        "# def display_gif(video_path_list, text_list):\n",
        "#     html = ''\n",
        "#     for video_path, text in zip(video_path_list, text_list):\n",
        "#         html_line = '<img src=\"{}\"> {} <br/>'.format(video_path, text)\n",
        "#         html += html_line\n",
        "#     return display.HTML(html)\n",
        "\n",
        "    \n",
        "# def convert_video2gif(video_path, output_gif_path, num_samples=16):\n",
        "#     frames = (\n",
        "#         towhee.glob(video_path)\n",
        "#               .video_decode.ffmpeg(sample_type='uniform_temporal_subsample', args={'num_samples': num_samples})\n",
        "#               .to_list()[0]\n",
        "#     )\n",
        "#     imgs = [Image.fromarray(frame) for frame in frames]\n",
        "#     imgs[0].save(fp=output_gif_path, format='GIF', append_images=imgs[1:], save_all=True, loop=0)\n",
        "\n",
        "\n",
        "# def display_gifs_from_video(video_path_list, text_list, tmpdirname = './tmp_gifs'):\n",
        "#     Path(tmpdirname).mkdir(exist_ok=True)\n",
        "#     gif_path_list = []\n",
        "#     for video_path in video_path_list:\n",
        "#         video_name = str(Path(video_path).name).split('.')[0]\n",
        "#         gif_path = Path(tmpdirname) / (video_name + '.gif')\n",
        "#         convert_video2gif(video_path, gif_path)\n",
        "#         gif_path_list.append(gif_path)\n",
        "#     return display_gif(gif_path_list, text_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "resources": {
            "http://localhost:8080/tmp_gifs/video7579.gif": {
              "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
              "headers": [
                [
                  "content-length",
                  "1449"
                ],
                [
                  "content-type",
                  "text/html; charset=utf-8"
                ]
              ],
              "ok": false,
              "status": 404,
              "status_text": ""
            },
            "http://localhost:8080/tmp_gifs/video7725.gif": {
              "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
              "headers": [
                [
                  "content-length",
                  "1449"
                ],
                [
                  "content-type",
                  "text/html; charset=utf-8"
                ]
              ],
              "ok": false,
              "status": 404,
              "status_text": ""
            },
            "http://localhost:8080/tmp_gifs/video9258.gif": {
              "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
              "headers": [
                [
                  "content-length",
                  "1449"
                ],
                [
                  "content-type",
                  "text/html; charset=utf-8"
                ]
              ],
              "ok": false,
              "status": 404,
              "status_text": ""
            }
          }
        },
        "id": "6imosALM04pX",
        "outputId": "1dfedd6e-b800-4d28-8ab3-d0794812474e"
      },
      "outputs": [],
      "source": [
        "# sample_show_df = sample_df[:3]\n",
        "# video_path_list = sample_show_df['video_path'].to_list()\n",
        "# text_list = sample_show_df['sentence'].to_list()\n",
        "# tmpdirname = './tmp_gifs'\n",
        "# display_gifs_from_video(video_path_list, text_list, tmpdirname=tmpdirname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "2GGKA2-N2AaH",
        "outputId": "89f198a3-40c1-48f6-f712-b6cca3c1e823"
      },
      "outputs": [],
      "source": [
        "# from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
        "\n",
        "# connections.connect(host='127.0.0.1', port='19530')\n",
        "\n",
        "# # def create_milvus_collection(collection_name, dim):\n",
        "# #     if utility.has_collection(collection_name):\n",
        "# #         utility.drop_collection(collection_name)\n",
        "    \n",
        "# #     fields = [\n",
        "# #     FieldSchema(name='id', dtype=DataType.INT64, descrition='ids', is_primary=True, auto_id=False),\n",
        "# #     FieldSchema(name='embedding', dtype=DataType.FLOAT_VECTOR, descrition='embedding vectors', dim=dim)\n",
        "# #     ]\n",
        "# #     schema = CollectionSchema(fields=fields, description='video retrieval')\n",
        "# #     collection = Collection(name=collection_name, schema=schema)\n",
        "\n",
        "# #     # create IVF_FLAT index for collection.\n",
        "# #     index_params = {\n",
        "# #         'metric_type':'L2', #IP\n",
        "# #         'index_type':\"IVF_FLAT\",\n",
        "# #         'params':{\"nlist\":2048}\n",
        "# #     }\n",
        "# #     collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
        "# #     return collection\n",
        "\n",
        "# # collection = create_milvus_collection('text_video_retrieval', 512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "H_qmdQi42uY-"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import towhee\n",
        "\n",
        "# # # device = 'cuda:0'\n",
        "# # # device = 'cpu'\n",
        "# # # For the first time you run this line, \n",
        "# # # it will take some time \n",
        "# # # because towhee will download operator with weights on backend.\n",
        "# # dc = (\n",
        "# #     towhee.read_csv(test_sample_csv_path)\n",
        "# #       .runas_op['video_id', 'id'](func=lambda x: int(x[-4:]))\n",
        "# #       .video_decode.ffmpeg['video_path', 'frames'](sample_type='uniform_temporal_subsample', args={'num_samples': 12})\n",
        "# #       .runas_op['frames', 'frames'](func=lambda x: [y for y in x])\n",
        "# #       .video_text_embedding.clip4clip['frames', 'vec'](model_name='clip_vit_b32', modality='video')\n",
        "# #       .to_milvus['id', 'vec'](collection=collection, batch=30)\n",
        "# # )\n",
        "# # print('Total number of inserted data is {}.'.format(collection.num_entities))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "AH2XgYH03XcJ"
      },
      "outputs": [],
      "source": [
        "# # device = 'cuda:0'\n",
        "# dc = (\n",
        "#     towhee.read_csv(test_sample_csv_path).unstream()\n",
        "#       .video_text_embedding.clip4clip['sentence','text_vec'](model_name='clip_vit_b32', modality='text')\n",
        "#       .milvus_search['text_vec', 'top10_raw_res'](collection='text_video_retrieval', limit=10)\n",
        "#       .runas_op['video_id', 'ground_truth'](func=lambda x : [int(x[-4:])])\n",
        "#       .runas_op['top10_raw_res', 'top1'](func=lambda res: [x.id for i, x in enumerate(res) if i < 1])\n",
        "#       .runas_op['top10_raw_res', 'top5'](func=lambda res: [x.id for i, x in enumerate(res) if i < 5])\n",
        "#       .runas_op['top10_raw_res', 'top10'](func=lambda res: [x.id for i, x in enumerate(res) if i < 10])\n",
        "# )\n",
        "\n",
        "# dc.select['video_id', 'sentence', 'ground_truth', 'top10_raw_res', 'top1', 'top5', 'top10']().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# benchmark = (\n",
        "#     dc.with_metrics(['mean_hit_ratio',])\n",
        "#         .evaluate['ground_truth', 'top1'](name='recall_at_1')\n",
        "#         .evaluate['ground_truth', 'top5'](name='recall_at_5')\n",
        "#         .evaluate['ground_truth', 'top10'](name='recall_at_10')\n",
        "#         .report()\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import gradio\n",
        "# import towhee.models.clip4clip\n",
        "\n",
        "# device = 'cuda:0'\n",
        "# collection='text_video_retrieval'\n",
        "# show_num = 3\n",
        "# with towhee.api() as api:\n",
        "#     milvus_search_function = (\n",
        "#          api.clip4clip(model_name='clip_vit_b32', modality='text', device=device)\n",
        "#             .milvus_search(collection=collection, limit=show_num)\n",
        "#             .runas_op(func=lambda res: [os.path.join(raw_video_path, 'video' + str(x.id) + '.mp4') for x in res])\n",
        "#             .as_function()\n",
        "#     )\n",
        "\n",
        "# interface = gradio.Interface(milvus_search_function, \n",
        "#                              inputs=[gradio.Textbox()],\n",
        "#                              outputs=[gradio.Video(format='mp4') for _ in range(show_num)]\n",
        "#                             )\n",
        "\n",
        "# interface.launch(inline=True, share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Towhee text-image inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# img_path_df = pd.DataFrame(columns=[\"img_path\",\"video_id\",\"frame\"])\n",
        "# img_path_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for video in os.listdir(\"../AIChallenge_data/KeyFrames\"):\n",
        "#     fix_frame_df = pd.read_csv(os.path.join(\"../AIChallenge_data/Fix_KeyFrames/\",video +\".csv\"),names=[\"img\",\"frame\"])\n",
        "#     for i in range(len(fix_frame_df)):\n",
        "#         img_path_df = img_path_df.append({\"img_path\":os.path.join(\"KeyFrames\",video,fix_frame_df.iloc[i][\"img\"]),\n",
        "#         \"video_id\":video,\"frame\":fix_frame_df.iloc[i][\"frame\"]},ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# img_path_df.to_csv(\"key_frame_df.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# a = \"./train/brain_coral/n01917289_1783.JPEG\"\n",
        "# a.encode(\"utf-8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
        "\n",
        "connections.connect(host='127.0.0.1', port='19530')\n",
        "\n",
        "def create_milvus_collection(collection_name, dim):\n",
        "    if utility.has_collection(collection_name):\n",
        "        utility.drop_collection(collection_name)\n",
        "    \n",
        "    fields = [\n",
        "    FieldSchema(name='id', dtype=DataType.INT64, descrition='ids', is_primary=True, auto_id=False),\n",
        "    FieldSchema(name='embedding', dtype=DataType.FLOAT_VECTOR, descrition='embedding vectors', dim=dim)\n",
        "    ]\n",
        "    schema = CollectionSchema(fields=fields, description='text image search')\n",
        "    collection = Collection(name=collection_name, schema=schema)\n",
        "\n",
        "    # create IVF_FLAT index for collection.\n",
        "    index_params = {\n",
        "        'metric_type':'L2',\n",
        "        'index_type':\"IVF_FLAT\",\n",
        "        'params':{\"nlist\":512}\n",
        "    }\n",
        "    collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
        "    return collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import towhee\n",
        "\n",
        "# towhee.glob['path'](img_path[101:102]) \\\n",
        "#       .image_decode['path', 'img']() \\\n",
        "#       .image_text_embedding.clip['img', 'vec'](model_name='clip_vit_b32', modality='image') \\\n",
        "#       .tensor_normalize['vec','vec']() \\\n",
        "#       .select['img', 'vec']() \\\n",
        "#       .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# towhee.dc['text']([\"A cat watching a computer screen while siting on a bench\"]) \\\n",
        "#       .image_text_embedding.clip['text','vec'](model_name='clip_vit_b32', modality='text') \\\n",
        "#       .tensor_normalize['vec','vec']() \\\n",
        "#       .select['text', 'vec']() \\\n",
        "#       .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "d:\\GitHub\\AIChallenge_data\n"
          ]
        }
      ],
      "source": [
        "%cd ../AIChallenge_data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_keyframes = pd.read_csv('key_frame_df.csv',index_col=0)\n",
        "\n",
        "import cv2\n",
        "from towhee._types.image import Image\n",
        "\n",
        "id_img = df_keyframes.set_index('id')['img_path'].to_dict()\n",
        "def read_images(results):\n",
        "    imgs = []\n",
        "    for re in results:\n",
        "        # print(re)\n",
        "        path = id_img[re.id]\n",
        "        imgs.append(path)\n",
        "    return imgs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# collection = create_milvus_collection('text_image_challenge_search', 512)\n",
        "collection = create_milvus_collection('text_image_search', 512)\n",
        "# collection = Collection('text_image_challenge_search')\n",
        "dc = (\n",
        "    towhee.read_csv('key_frame_df.csv')\n",
        "      .runas_op['id', 'id'](func=lambda x: int(x))\n",
        "      .set_parallel(4)\n",
        "      .image_decode['img_path', 'img']()\n",
        "      .image_text_embedding.clip['img', 'vec'](model_name='clip_vit_b32', modality='image')\n",
        "      .tensor_normalize['vec','vec']()\n",
        "      .to_milvus['id', 'vec'](collection='text_image_search', batch=100)\n",
        "      \n",
        ")\n",
        "print('Total number of inserted data is {}.'.format(collection.num_entities))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# cd reverse_image_search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# query = \"The video is a continuous sketch of a foreign criminal. The first picture depicts a criminal and two policemen sitting in the back. The second picture has 14 people, including one person wearing a black shirt.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "queries = {}\n",
        "with open(\"./query.txt\",\"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "        query_name = line.split(\"//\")[0]\n",
        "        query = line.split(\"//\")[1]\n",
        "        queries[query_name] = query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# for i in range(26,len(queries.keys())):\n",
        "#   key = list(queries.keys())[i]\n",
        "#   (\n",
        "#       towhee.dc['text']([queries[key]])\n",
        "#         .image_text_embedding.clip['text', 'vec'](model_name='clip_vit_b32', modality='text')\n",
        "#         .tensor_normalize['vec','vec']()\n",
        "#         .milvus_search['vec', 'result'](collection=collection, limit=100)\n",
        "#         .runas_op['result', 'result_img'](func=read_images)\n",
        "#         .select['result_img']()\n",
        "#         .to_csv(key+\".csv\")      \n",
        "#   )\n",
        "#   print(i)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Bản tin về một trận đấu của đội tuyển bóng đá Việt Nam. Các cầu thủ Việt Nam đang ăn mừng sau một bàn thắng. Đây không phải là đội tuyển quốc gia mà là đội tuyển U19. Bàn thắng nâng tỷ số lên 2-0 xuất phát từ một pha đánh đầu cận thành. Đối thủ của U19 Việt Nam là U19 Brunei.',\n",
              " 'Một hoạt động giải trí ưa thích của giới trẻ. Thả diều tại ngoại ô TPHCM. Trong bức hình có một chiếc diều khổng lồ hình con mực. Cùng ngày là phóng sự khởi công cầu Châu Đốc bắc qua song Hậu.',\n",
              " 'Một cuộc đua xe đồng đội, gồm người lớn và trẻ em. Những chiếc xe này không có động cơ, thay vào đó chúng được đẩy bởi các vị phụ huynh. Tại vạch xuất phát, có 3 người mặc đồ hóa trang thành những chú ngựa vằn. Bản tin này được phát vào ngày mà hôm trước đó là ngày Quốc khánh của nước Thụy Sĩ.',\n",
              " 'Một sở thú tại Trung Quốc với nhiều loài động vật khác nhau. Trong khung hình có 2 con hà mã. Một trong 2 đang uống nước. Ngoài hà mã, sở thú còn có voi và gấu trúc.',\n",
              " 'Một bức hình của Tổng thống Mỹ Joe Biden. Tổng thống Mỹ đang đeo kính đen. Đó là trang nhất của một bài báo. Tiêu đề tiếng Việt ghi: Mỹ có thể sẽ miễn thuế pin năng lượng mặt trời từ Việt Nam. Đó là một bài báo từ hãng tin Reuters.',\n",
              " 'Người nghệ nhân đang tô màu cho chiếc mặt nạ một cách tỉ mỉ. Xung quanh ông là rất nhiều những chiếc mặt nạ. Người nghệ nhân đi đôi dép tổ ong rất giản dị. Loại mặt nạ này được gọi là mặt nạ giấy bồi Trung thu.',\n",
              " 'Khung cảnh chứa 3-4 người trong những bộ trang phục sáng bóng. Trước đó là hình ảnh một đội quân xếp hàng, giống như một nghi lễ. Đây là những thành viên của Vệ binh Thụy Sĩ. Họ đang thực hiện lời tuyên thệ bảo vệ Giáo Hoàng.',\n",
              " 'Hình ảnh một người đàn ông cầm một bó hoa to. Anh đang ở một cửa hàng hoa. Cửa hàng này có tên là “Flower box”. Hình ảnh này nằm trong môt phóng sự về giá hoa hồng trong ngày Valentine.',\n",
              " 'Một người đàn ông đang chơi kèn saxophone. Đằng sau ông là một người khác đang chơi viola. Ngoài ra còn có một người chơi trống, và một cây dương cầm. Bốn nhạc cụ tạo thành một tứ tấu jazz (jazz quartet). Đây là một phần của sự kiện nhằm thắt chặt tình hữu nghị giữa nhóm các nước V4 với Việt Nam.',\n",
              " 'Một bức tượng của một nhân vật lịch sử nổi tiếng. Nhân vật này đang cưỡi ngựa. Tiêu đề tiếng Việt ghi: “Thẩm định tượng Đức Thánh Trần ở Hồ Mây, Vũng Tàu”. Bức tượng có màu vàng đồng.']"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query_dir = \"D:\\GitHub\\AIChallenge_data\\query-pack-0\\query-pack-0\"\n",
        "list_queries_file = os.listdir(query_dir)\n",
        "vi_queries = []\n",
        "for query in list_queries_file:\n",
        "    with open(os.path.join(query_dir,query),\"r\",encoding='utf8') as f:\n",
        "        read_line = f.readline()\n",
        "        vi_queries.append(read_line)\n",
        "\n",
        "vi_queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['The news of a match of Vietnamese football team. Vietnamese players are celebrating after a goal. This is not the national team but the U19 team. The goal that lifted the score to 2-0 came from a close-fitting header. Opponents of Vietnam U19 are Brunei U19.',\n",
              " 'A favorite recreational activity of young people. Flying kites in the suburbs of Ho Chi Minh City. In the picture there is a giant kite in the shape of a squid. On the same day was the launch of Chau Doc Bridge across the Hau parallel.',\n",
              " 'A team car race, consisting of adults and children. These cars do not have engines, instead they are pushed by the parents. At the starting line, there are 3 people dressed up as zebras. This newscast was broadcast on the day that the day before was the Swiss National Day.',\n",
              " 'A zoo in China with many different animals. In the frame there are 2 hippos. One of the 2 is drinking water. In addition to hippos, the zoo also has elephants and raccoons.',\n",
              " 'A picture of U.S. President Joe Biden. The U.S. President is wearing black glasses. It was the front page of an article. The Vietnamese title read: The U.S. may waive tariffs on solar cells from Vietnam. It was an article from Reuters news agency.',\n",
              " 'The artist is coloring the mask meticulously. Around him are a lot of masks. The artist wears very simple honeycomb slippers. This type of mask is called a Mid-Autumn paper mask.',\n",
              " 'The scene contains 3-4 people in shiny outfits. Before that is the image of an army lined up, which resembles a ritual. These are members of the Swiss Guard. They are taking the oath to protect the Pope.',\n",
              " 'Picture of a man holding a large bouquet of flowers. He is at a flower shop. This shop is called “Flower box\". This image is in a reportage about the price of roses on Valentine\\'s Day.',\n",
              " 'A man is playing the saxophone. Behind him is another man playing the viola. There is also a drummer, and a piano. The four instruments form a jazz quartet. This is part of the event to strengthen the friendship between the group of V4 countries with Vietnam.',\n",
              " 'A statue of a famous historical figure. This figure is riding a horse. The Vietnamese title reads: “Tham đinh tuong Đuc Thanh Tran o Ho May, Vung Tau\". The statue is bronze yellow.']"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eng_queries = [translate_vi2en(x) for x in vi_queries]\n",
        "eng_queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [],
      "source": [
        "(\n",
        "      towhee.dc['text'](eng_queries)\n",
        "        .image_text_embedding.clip['text', 'vec'](model_name='clip_vit_b32', modality='text')\n",
        "        .tensor_normalize['vec','vec']()\n",
        "        .milvus_search['vec', 'result'](collection=collection, limit=100)\n",
        "        .runas_op['result', 'result_img'](func=read_images)\n",
        "        .select['result_img']()\n",
        "        .to_csv(\"test\"+\".csv\")  \n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ast import literal_eval\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"./test.csv\")\n",
        "df.result_img =df.result_img.apply(literal_eval)\n",
        "\n",
        "for i in range(len(list_queries_file)):\n",
        "    with open(\"{}_result.txt\".format(list_queries_file[i]),\"w\") as f:\n",
        "        list_img = df[\"result_img\"][i]\n",
        "        for img in list_img:\n",
        "            video_id = img.split(\"\\\\\")[-2]\n",
        "            false_frame = img.split(\"\\\\\")[-1].split(\".\")[0]\n",
        "            real_frame = df_keyframes[df_keyframes.false_frame == int(false_frame)][video_id==df_keyframes.video_id].frame.values[0]\n",
        "            f.write(video_id + \".mp4, \" + str(real_frame) + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "# with towhee.api() as api:\n",
        "#     milvus_search_function = (\n",
        "#         api.image_text_embedding.clip(model_name='clip_vit_b32',modality='text')\n",
        "#             .tensor_normalize()\n",
        "#             .milvus_search(collection='text_image_search', limit=5)\n",
        "#             .runas_op(func=lambda res: [id_img[x.id] for x in res])\n",
        "#             .as_function()\n",
        "#     )\n",
        "\n",
        "# import gradio\n",
        "\n",
        "# interface = gradio.Interface(milvus_search_function, \n",
        "#                              gradio.inputs.Textbox(lines=1),\n",
        "#                              [gradio.outputs.Image(type=\"file\", label=None) for _ in range(5)]\n",
        "#                             )\n",
        "\n",
        "# interface.launch(inline=True, share=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Calculate score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6.4"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "rank = [18,5,1,1,2,33,101,63,1,13]\n",
        "\n",
        "\n",
        "accept_rank = [1,5,20,50,100]\n",
        "score = []\n",
        "for i in range(len(rank)):\n",
        "    for j in range(len(accept_rank)):\n",
        "        score.append(1 if rank[i] <= accept_rank[j] else 0)\n",
        "\n",
        "total_score = np.sum(score) / 5\n",
        "total_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyO+dEP2VUwEhYEC4xULUEzS",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.12 ('env4ml')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "f684d485204dfdeef2abe219ca21370c20995e9c270e0d9ebbd2a2aeed1acfbd"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
